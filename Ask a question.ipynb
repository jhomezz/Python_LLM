{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "# from IPython.display import Markdown, display\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "\n",
    "# If you get an error running this cell, then please head over to the troubleshooting notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "# MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling dde5aa3fc5ff... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 2.0 GB                         \n",
      "pulling 966de95ca8a6... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 1.4 KB                         \n",
      "pulling fcc5a6bec9da... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 7.7 KB                         \n",
      "pulling a70ff7e570d9... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 6.0 KB                         \n",
      "pulling 56bb8bd477a5... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�   96 B                         \n",
      "pulling 34bb5ab01051... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�  561 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "\n",
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n",
    "\n",
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88811402-78be-4734-af77-15756d62aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "# If this doesn't work, try Kernel menu >> Restart Kernel and Clear Outputs Of All Cells, then run the cells from the top of this notebook down.\n",
    "# If it STILL doesn't work (horrors!) then please see the Troubleshooting notebook in this folder for full instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "system_prompt =\"\"\"\n",
    "You are a patient tutor for a beginner programmer.  Please use multiple examples and analogies to explain the answer.\n",
    "\"\"\"\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bed78cf-9fab-46bb-b0d4-c2da2c1d5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "567174c2-36e0-4090-a829-22c1d8a75fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL_LLAMA,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "def stream_response(system_prompt, question):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "          ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f72f8310-6589-408a-9804-49be57e3ce28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Let's break down this line of code step by step so you understand what it does and why it is written that way.\n",
       "\n",
       "### Code Breakdown\n",
       "1. **Set Comprehension**:\n",
       "   - The expression `{book.get(\"author\") for book in books if book.get(\"author\")}` is a **set comprehension**. This is similar to list comprehensions but is used to create a set, which automatically removes duplicate values.\n",
       "   - The `for book in books` part means we are looping through each `book` in a collection called `books`.\n",
       "   - `book.get(\"author\")` fetches the value associated with the key `\"author\"` from the `book` dictionary.\n",
       "   - The `if book.get(\"author\")` part serves as a filter. It checks whether the `author` exists and is not `None` (or otherwise \"falsy\"). If it does, then we get that author's name; if not, we skip to the next book.\n",
       "\n",
       "   **Example**: Let's say you have a list of books like this:\n",
       "   python\n",
       "   books = [\n",
       "       {\"title\": \"Book A\", \"author\": \"Author A\"},\n",
       "       {\"title\": \"Book B\"},\n",
       "       {\"title\": \"Book C\", \"author\": \"Author B\"},\n",
       "       {\"title\": \"Book D\", \"author\": \"Author A\"},\n",
       "   ]\n",
       "   \n",
       "   - The code generates a set of authors:\n",
       "   python\n",
       "   {'Author A', 'Author B'}\n",
       "   \n",
       "   This happens because `Author A` appears twice but is only listed once in the set.\n",
       "\n",
       "2. **Yield From**:\n",
       "   - The `yield from` keyword is used to delegate part of a generator’s operations to another generator. This means that instead of returning a single value, it allows a generator to yield multiple values from another iterable.\n",
       "   - In this case, `yield from` will yield each author from the previously created set, one at a time.\n",
       "\n",
       "### Putting it Together\n",
       "So, the whole line of code essentially:\n",
       "- Creates a set of unique author names from a list of book dictionaries, filtering out any books that do not have an author.\n",
       "- Yields each unique author name one at a time.\n",
       "\n",
       "### Analogy\n",
       "If we think of this in more relatable terms:\n",
       "- Imagine you're at a library and you are asked to list down all the unique authors for the books present.\n",
       "- You walk through each book (the list of dictionaries) and write down the author if the book has one (the filter).\n",
       "- As you write down the names, you notice you already wrote \"Author A\" recently, so you skip writing it again (the set, which avoids duplicates).\n",
       "- In the end, instead of handing over the entire list at once, you choose to call out each name one by one (using `yield from`).\n",
       "\n",
       "### Summary\n",
       "In short, this line of code is useful for gathering a unique list of authors from a collection of book records and yields them one at a time for further processing. If you have any more questions or need further clarification, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_response(system_prompt, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This line of code is written in Python and utilizes a feature called \"yield from\" which allows the function to delegate iteration to another iterable.\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "- `for book in books`: This part iterates over each item (book) in the `books` collection.\n",
      "\n",
      "- `.get(\"author\") for book in books if book.get(\"author\")`: For each book, this part tries to get the value of the key `\"author\"`. If the key doesn't exist (`None` is returned), it skips that book. This creates a generator expression which produces an iterable sequence.\n",
      "\n",
      "- `yield from ...`: The `yield from` statement takes the sequence produced by the generator expression and yields each item directly, without creating a new container to hold them all.\n",
      "\n",
      "So overall, this line of code will produce a sequence of books where the author is not missing (`None`), one book at a time, as if it came from the original iterable.\n",
      "\n",
      "The benefits are:\n",
      "\n",
      "1. More memory-efficient: Because we're not storing the entire sequence in memory at once, this approach can be more efficient for large datasets.\n",
      "2. Faster iteration: We only wait until each item is actually available, rather than waiting for all of them to finish being generated.\n",
      "\n",
      "Example use case:\n",
      "```python\n",
      "books = [\n",
      "    {\"id\": 1, \"title\": \"The Great Gatsby\", \"author\": \"F. Scott Fitzgerald\"},\n",
      "    {\"id\": 2, \"title\": \"Pride and Prejudice\", \"author\": None},\n",
      "    {\"id\": 3, \"title\": \"To Kill a Mockingbird\", \"author\": \"Harper Lee\"}\n",
      "]\n",
      "\n",
      "for author in yield from {book.get(\"author\") for book in books if book.get(\"author\")}:\n",
      "    print(author)\n",
      "```\n",
      "This will output:\n",
      "```\n",
      "F. Scott Fitzgerald\n",
      "Harper Lee\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Get Llama 3.2 # If this doesn't work for any reason, try the 2 versions in the following cells\n",
    "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
    "# And if none of that works - contact me!\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
